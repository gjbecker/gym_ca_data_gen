# reward implemented in gym_collision_avoidance/envs/collision_avoidance_env.py

[default]
# reward given when agent reaches goal position
reach_goal = 1.0
# reward given when agent collides with another agent
collision_agent = -0.25
# reward given when agent collides with wall (unused, set USE_STATIC_MAP = True)
collision_wall = -0.25
# reward when agent gets close to another agent
close_reward = -0.1
close_range = 0.2
# default reward given if none of the others apply (encourage speed with neg)
timestep = 0.0
# (ADDED) reward function modeled after reacher env
reacher = False

[close_range_inc]
reach_goal = 1.0
collision_agent = -0.25
collision_wall = -0.25
close_reward = -0.1
close_range = 0.4
timestep = 0.0
reacher = False

[existential]
reach_goal = 1.0
collision_agent = -0.25
collision_wall = -0.25
close_reward = -0.1
close_range = 0.2
timestep = 1.0
reacher = False

[reacher]
reach_goal = 1.0
collision_agent = -0.25
collision_wall = -0.25
close_reward = -0.1
close_range = 0.2
timestep = 0.0
# Only thing that matters
reacher = True